{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5fa78817",
      "metadata": {
        "id": "5fa78817"
      },
      "source": [
        "# Analyse outillée d'une correspondance en XML-TEI\n",
        "\n",
        "Ce notebook vise à présenter quelques méthodes d'analyse computationnelle, d'encodage automatique et de visualisation de données. D'un point de vue technique, cet atelier introduit au XML-TEI, et surtout à son analyse outillée avec Python:\n",
        "- lecture, extraction d'informations, construction et écriture de fichiers XML (librairie `lxml`)\n",
        "- enrichissement automatique par la récupération d'informations géographiques via des API (librairie `requests`)\n",
        "- extraction d'informations: classification automatisée d'entités nommées (TAL, avec `spacy`)\n",
        "- visualisation en réseau du corpus (librairie `pyvis`)\n",
        "- cartographie interactive (librairie `folium`).\n",
        "\n",
        "Le corpus utilisé pour cet atelier est un ensemble de correspondances autour de l'achat par Matsukata d'un ensembles d'œuvres européennes, notamment par l'intermédiaire de Léonce Bénédite, durant la première moitié du XIXe siècle. Les originaux sont conservés dans les archives de l'Institut national d'histoire de l'art et du Musée Rodin. La version numérique du corpus a été produite par Léa Saint-Raymond (production d'une version en texte brut avec indexation des métadonnées). À partir de là, un encodage automatique a été réalisé en XML-TEI, à l'aide de Python (voir le script [txt2xml.py](https://github.com/paulhectork/cours_ens2023_xmltei/blob/main/src/txt2xml.py)).\n",
        "\n",
        "À partir de ce corpus, nous allons:\n",
        "- extraire des informations géographiques (lieu d'expédition/réception) et les géocoder à l'aide de données d'Openstreetmap. On se servira de ces données pour produire un `settingDesc` dans l'`encodingDesc` du `teiHeader` de chaque fichier XML.\n",
        "- identifier certaines entités (personnes et organisations expéditrices/destinataires de lettres du corpus), et classer celles-ci à l'aide de de Spacy (reconnaissance d'entités nommées, apprentissage machine). À partir de là, on construira un `particDesc` qui documente tou.te.s les expéditeur.ice.s et destinataires de lettres du corpus.\n",
        "- produire une visualisation en réseau de ces expéditeur.ice.s et destinataires, en faisant de la fouille de texte du corpus de fichiers XML\n",
        "- produire une cartographie interactive des villes d'expédition/réception de lettres du corpus, là encore en faisant de la fouille de texte.\n",
        "\n",
        "Tout un programme donc!\n",
        "\n",
        "---\n",
        "\n",
        "## La structure des fichiers XML\n",
        "\n",
        "Les fichiers XML produits à la fin de l'exercice ressembleront à l'exemple ci-dessous. Plusieurs éléments ne sont pas présents dans les fichiers au début de l'exercice, et surtout les `settingDesc` et `particDesc`.\n",
        "\n",
        "```xml\n",
        "<TEI xmlns=\"http://www.tei-c.org/ns/1.0\" \n",
        "     xml:id=\"KojiroMatsukata_L&#233;onceB&#233;n&#233;dite_19200619_4832865354280595070\">\n",
        "  <!-- @xmlns: l'escpace de nom de la TEI -->\n",
        "  <!-- @xml:id: l'identifiant de notre fichier, qui correspond avec le nom de celui-ci -->\n",
        "  <teiHeader>\n",
        "    <!-- teiHeader: l'en-t&#234;te de notre document, contenant les m&#233;tadonn&#233;es -->\n",
        "    <fileDesc>\n",
        "      <!-- fileDesc: description du fichier encod&#233; et de sa source -->\n",
        "      <titleStmt>\n",
        "        <!-- titleStmt: informations sur le titre du document -->\n",
        "        <title>Lettre typographi&#233;e sur papier &#224; en-t&#234;te Kawasaki, Kobe, de Matsukata &#224; Leone </title>\n",
        "        <author>Kojiro Matsukata</author>\n",
        "        <respStmt>\n",
        "          <!-- respStmt: qui sont les responsables de la production de l'encodage numérique -->\n",
        "          <resp>Production et pr&#233;paration du texte brut</resp>\n",
        "          <persName>L&#233;a Saint-Raymond</persName>\n",
        "        </respStmt>\n",
        "        <respStmt>\n",
        "          <resp>Transformation automatique du texte brut vers le XML-TEI</resp>\n",
        "          <persName>Les participant.e.s &#224; l'atelier \"Mod&#233;liser et exploiter des corpus textuels\" (ENS-PSL, campus d'Ulm)</persName>\n",
        "        </respStmt>\n",
        "      </titleStmt>\n",
        "      <publicationStmt>\n",
        "        <!-- piblicationStmt: informations sur le document XML encodé -->\n",
        "        <publisher>ENS-PSL</publisher>\n",
        "        <pubPlace>Paris (France)</pubPlace>\n",
        "        <date>2023-03-26 16:02:14.447940</date>\n",
        "      </publicationStmt>\n",
        "      <sourceDesc>\n",
        "        <!-- sourceDesc: informations sur la source \n",
        "             (l'original papier, la lettre que l'on encode) -->\n",
        "        <bibl type=\"lettre\">\n",
        "          <author>Kojiro Matsukata</author>\n",
        "          <title>Lettre typographi&#233;e sur papier &#224; en-t&#234;te \n",
        "              Kawasaki, Kobe, de Matsukata &#224; Leone </title>\n",
        "          <date>1920-06-19</date>\n",
        "          <msIdentifier>\n",
        "            <institution>INHA</institution>\n",
        "            <idno>INHA 56</idno>\n",
        "          </msIdentifier>\n",
        "        </bibl>\n",
        "      </sourceDesc>\n",
        "    </fileDesc>\n",
        "    <encodingDesc>\n",
        "      <!-- informations sur l'encodage :\n",
        "           - tei:editorialDecl, décrivant la méthode de production du document XML \n",
        "           - tei:projectDesc, contexte de production du XML \n",
        "      -->\n",
        "      <editorialDecl>\n",
        "        <p>Production de l'encodage XML r&#233;alis&#233;e automatiquement avec la librairie LXML de Python &#224; partir d'une version en texte brut de la correspondance de Matsukata</p>\n",
        "      </editorialDecl>\n",
        "      <projectDesc>\n",
        "        <p>L'atelier \"Mod&#233;liser et exploiter des corpus textuels\" a donn&#233; lieu &#224; cet encodage.</p>\n",
        "      </projectDesc>\n",
        "    </encodingDesc>\n",
        "    <profileDesc>\n",
        "      <!-- informations non bibliographiques sur le document encod&#233; \n",
        "           cet élément sera complété au fil du notebook par:\n",
        "           - un tei:particDesc, qui décrit tout.es les personnes ayant écrit\n",
        "             ou reçu des lettres dans le corpus Matsutaka\n",
        "           - un tei:settingDesc, qui décrit tous les lieux d'expédition/destination\n",
        "             des lettres du corpus Matsutaka\n",
        "           ces éléments seront rajoutés dans le notebook du cours.\n",
        "      -->\n",
        "      <correspDesc>\n",
        "        <!-- correspAction: description de la correspondance -->\n",
        "        <correspAction type=\"sent\">\n",
        "          <persName ref=\"#kojiromatsukata\">Kojiro Matsukata</persName>\n",
        "          <placeName ref=\"#kobe\">Kobe</placeName>\n",
        "          <date when=\"1920-06-19\">1920-06-19</date>\n",
        "        </correspAction>\n",
        "        <correspAction type=\"received\">\n",
        "          <persName ref=\"#leoncebenedite\">L&#233;once B&#233;n&#233;dite</persName>\n",
        "          <placeName ref=\"#paris\">Paris</placeName>\n",
        "        </correspAction>\n",
        "      </correspDesc>\n",
        "      <settingDesc>\n",
        "        <!-- le listPlace sera créé pendant le notebook: il décrit \n",
        "             les lieux  avec la structure suivante: -->\n",
        "        <listPlace>\n",
        "          <place xml:id=\"paris\">\n",
        "            <placeName>Paris</placeName>\n",
        "            <location>\n",
        "              <geo>2.3483915 48.8534951</geo>\n",
        "            </location>\n",
        "          </place>\n",
        "          <!-- ... -->\n",
        "        </listPlace>\n",
        "        -->\n",
        "      </settingDesc>\n",
        "      <particDesc>\n",
        "        <!-- pareil, le particDesc sera créé au fil du notebook. il aura une structure\n",
        "             équivalente: -->\n",
        "        <listPerson>\n",
        "          <person type=\"PERSON\" xml:id=\"leoncebenedite\">\n",
        "            <persName>L&#233;once B&#233;n&#233;dite</persName>\n",
        "            <persName>L&#233;once B&#233;n&#233;dite ?</persName>\n",
        "          </person>\n",
        "          <!-- ... -->\n",
        "        </listPerson>\n",
        "        <listOrg>\n",
        "          <org type=\"ORG\" xml:id=\"compagniealgerienne\">\n",
        "            <orgName>Compagnie alg&#233;rienne</orgName>\n",
        "          </org>\n",
        "          <!-- ... -->\n",
        "        </listOrg>\n",
        "      </particDesc>\n",
        "    </profileDesc>\n",
        "  </teiHeader>\n",
        "  <text>\n",
        "    <!-- le corps du texte. il peut comporter un tei:front et un tei:back\n",
        "         et doit comporter un tei:body, comprenant le corps du texte -->\n",
        "    <body>\n",
        "      <opener>\n",
        "        <!-- le corps du texte -->\n",
        "        <salute>Dear Sir, </salute>\n",
        "      </opener>\n",
        "      <p><!-- la lettre en tant que telle --></p>\n",
        "      <closer>\n",
        "        <!-- la fermeture -->\n",
        "        <salute>Yours faithfully, </salute>\n",
        "        <signed>Kojiro Matsukata</signed>\n",
        "      </closer>\n",
        "    </body>\n",
        "  </text>\n",
        "</TEI>\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Les bases\n",
        "\n",
        "Pour bien faire notre travail, on commence par **importer les librairies**. Une librairie est un ensemble de fonctions avec une finalité spécifique: visualisation, fouille de texte... Certaines viennent par défaut, d'autres sont conçues par des tiers pour augmenter les fonctionnalités de Python et doivent être installées (ce qu'on a fait avec `pip install`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installer les dépendances. ça va prendre du temps\n",
        "!pip install asttokens==2.2.1\n",
        "!pip install backcall==0.2.0\n",
        "!pip install blis==0.7.9\n",
        "!pip install branca==0.6.0\n",
        "!pip install catalogue==2.0.8\n",
        "!pip install certifi==2022.12.7\n",
        "!pip install charset-normalizer==3.1.0\n",
        "!pip install click==8.1.3\n",
        "!pip install cmake==3.26.1\n",
        "!pip install confection==0.0.4\n",
        "!pip install cymem==2.0.7\n",
        "!pip install decorator==5.1.1\n",
        "!pip install en-core-web-trf@https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.5.0/en_core_web_trf-3.5.0-py3-none-any.whl\n",
        "!pip install executing==1.2.0\n",
        "!pip install filelock==3.10.7\n",
        "!pip install folium==0.14.0\n",
        "!pip install fr-dep-news-trf@https://github.com/explosion/spacy-models/releases/download/fr_dep_news_trf-3.5.0/fr_dep_news_trf-3.5.0-py3-none-any.whl\n",
        "!pip install huggingface-hub==0.13.3\n",
        "!pip install idna==3.4\n",
        "!pip install ipython==8.11.0\n",
        "!pip install jedi==0.18.2\n",
        "!pip install Jinja2==3.1.2\n",
        "!pip install jsonpickle==3.0.1\n",
        "!pip install langcodes==3.3.0\n",
        "!pip install langdetect==1.0.9\n",
        "!pip install lit==16.0.0\n",
        "!pip install lxml==4.9.2\n",
        "!pip install MarkupSafe==2.1.2\n",
        "!pip install matplotlib-inline==0.1.6\n",
        "!pip install mpmath==1.3.0\n",
        "!pip install murmurhash==1.0.9\n",
        "!pip install networkx==3.0\n",
        "!pip install numpy==1.24.2\n",
        "!pip install nvidia-cublas-cu11==11.10.3.66\n",
        "!pip install nvidia-cuda-cupti-cu11==11.7.101\n",
        "!pip install nvidia-cuda-nvrtc-cu11==11.7.99\n",
        "!pip install nvidia-cuda-runtime-cu11==11.7.99\n",
        "!pip install nvidia-cudnn-cu11==8.5.0.96\n",
        "!pip install nvidia-cufft-cu11==10.9.0.58\n",
        "!pip install nvidia-curand-cu11==10.2.10.91\n",
        "!pip install nvidia-cusolver-cu11==11.4.0.1\n",
        "!pip install nvidia-cusparse-cu11==11.7.4.91\n",
        "!pip install nvidia-nccl-cu11==2.14.3\n",
        "!pip install nvidia-nvtx-cu11==11.7.91\n",
        "!pip install packaging==23.0\n",
        "!pip install parso==0.8.3\n",
        "!pip install pathy==0.10.1\n",
        "!pip install pexpect==4.8.0\n",
        "!pip install pickleshare==0.7.5\n",
        "!pip install preshed==3.0.8\n",
        "!pip install prompt-toolkit==3.0.38\n",
        "!pip install protobuf==3.20.3\n",
        "!pip install ptyprocess==0.7.0\n",
        "!pip install pure-eval==0.2.2\n",
        "!pip install pydantic==1.10.7\n",
        "!pip install Pygments==2.14.0\n",
        "!pip install pyvis==0.3.2\n",
        "!pip install PyYAML==6.0\n",
        "!pip install regex==2023.3.23\n",
        "!pip install requests==2.28.2\n",
        "!pip install sentencepiece==0.1.97\n",
        "!pip install six==1.16.0\n",
        "!pip install smart-open==6.3.0\n",
        "!pip install spacy==3.5.1\n",
        "!pip install spacy-alignments==0.9.0\n",
        "!pip install spacy-legacy==3.0.12\n",
        "!pip install spacy-loggers==1.0.4\n",
        "!pip install spacy-transformers==1.2.2\n",
        "!pip install srsly==2.4.6\n",
        "!pip install stack-data==0.6.2\n",
        "!pip install sympy==1.11.1\n",
        "!pip install thinc==8.1.9\n",
        "!pip install tokenizers==0.13.2\n",
        "!pip install torch==2.0.0\n",
        "!pip install tqdm==4.65.0\n",
        "!pip install traitlets==5.9.0\n",
        "!pip install transformers==4.26.1\n",
        "!pip install triton==2.0.0\n",
        "!pip install typer==0.7.0\n",
        "!pip install typing_extensions==4.5.0\n",
        "!pip install Unidecode==1.3.6\n",
        "!pip install urllib3==1.26.15\n",
        "!pip install wasabi==1.1.1\n",
        "!pip install wcwidth==0.2.6"
      ],
      "metadata": {
        "id": "4z0tDNuWnm98"
      },
      "id": "4z0tDNuWnm98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860efcb7",
      "metadata": {
        "id": "860efcb7"
      },
      "outputs": [],
      "source": [
        "from pyvis.network import Network  # réseaux\n",
        "from langdetect import detect      # détection de la langue\n",
        "from statistics import mode        # statistique\n",
        "from zipfile import ZipFile        # zipper / dézipper des fichiers\n",
        "from lxml import etree             # traiter du xml\n",
        "import unidecode                   # opérations sur du texte\n",
        "import requests                    # requêtes HTTP sur le Web\n",
        "import folium                      # cartographie\n",
        "import shutil                      # déplacements de fichiers\n",
        "import spacy                       # traitement automatisé des langues\n",
        "import json                        # traitement des fichiers json\n",
        "import time                        # le temps\n",
        "import math                        # maths (obviously)\n",
        "import sys                         # opérations sur le système d'exploitation\n",
        "import re                          # expressions régulières\n",
        "import os                          # opérations sur les fichiers et le système d'exploitation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fb63ac4",
      "metadata": {
        "id": "6fb63ac4"
      },
      "source": [
        "Ensuite, on **définit nos variables globales**. Celles-ci ont différents usages (principalement des chemins de fichiers), ne seront pas modifiées, et seront utilisées à différents endroits de notre code. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87612dae",
      "metadata": {
        "id": "87612dae"
      },
      "outputs": [],
      "source": [
        "# CURDIR = os.path.abspath(os.path.dirname(\"\"))                                  # dossier actuel\n",
        "# TXT = os.path.abspath(os.path.join(CURDIR, os.pardir, \"txt\"))                  # dossier `txt/`\n",
        "WEB = os.path.abspath(os.path.join(\"/\", \"content\", \"web\"))                       # dossier `web/`\n",
        "XML = os.path.abspath(os.path.join(\"/\", \"content\", \"xml\"))                       # dossier `xml/`\n",
        "UNZIP = os.path.abspath(os.path.join(XML, \"unzip\"))                              # dossier `xml/unzip`\n",
        "NS_TEI = {\"tei\": \"http://www.tei-c.org/ns/1.0\"}                                  # tei namespace\n",
        "NS_XML = {\"id\": \"http://www.w3.org/XML/1998/namespace\"}                          # general xml namespace\n",
        "TEI_RNG = \"https://tei-c.org/release/xml/tei/custom/schema/relaxng/tei_all.rng\"  # odd in .rng to validate tei files\n",
        "PARSER = etree.XMLParser(remove_blank_text=True)                                 # parser xml custom\n",
        "COLORS = { \"green\": \"#8fc7b1\",                                                   # codes couleurs html \n",
        "           \"gold\": \"#da9902\", \n",
        "           \"plum\": \"#710551\", \n",
        "           \"darkgreen\": \"#00553e\" }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fd4978",
      "metadata": {
        "id": "47fd4978"
      },
      "source": [
        "On **crée nos dossiers d'entrée et de sortie** pour ce faire, on utilise la librairie OS, à la syntaxe peu élégante, mais très pratique:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9323b0",
      "metadata": {
        "id": "8c9323b0"
      },
      "outputs": [],
      "source": [
        "# les chemins de fichiers sont exprimés depuis la base du dépôt, soit depuis le dossier parent\n",
        "# si le dossier `xml/unzip` n'existe pas, on le crée\n",
        "if not os.path.isdir(UNZIP):\n",
        "    os.makedirs(UNZIP)\n",
        "# si le dossier `web/` et `web/json` n'existent pas, on les crée\n",
        "if not os.path.isdir(os.path.join(WEB, \"json\")):\n",
        "    os.makedirs(os.path.join(WEB, \"json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a39e964",
      "metadata": {
        "id": "4a39e964"
      },
      "source": [
        "On **dézippe nos fichiers XML**. Ceux-ci sont embarqués dans une archive Zip pour faciliter l'atelier (et surtout pour l'utilisation de Google Colab). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71ce5c4",
      "metadata": {
        "id": "d71ce5c4"
      },
      "outputs": [],
      "source": [
        "# on dézippe nos fichiers\n",
        "with ZipFile( os.path.join(XML, \"corpus_matsutaka.zip\"), mode=\"r\" ) as zip:\n",
        "    zip.extractall(path=UNZIP)\n",
        "\n",
        "# on crée une dernière variable globale: une liste de tous les chemins absolus\n",
        "# vers les fichiers XML encodés. La liste permettra d'accéder aux fichiers.\n",
        "CORPUS = [ os.path.join(UNZIP, f) for f in os.listdir(UNZIP) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5aab3b",
      "metadata": {
        "id": "9c5aab3b"
      },
      "source": [
        "On crée enfin une fonction qui permette de **créer un @xml:id** à partir d'une chaîne de caractères. Elle sera utilisée à plusieurs points du corpus.\n",
        "\n",
        "Pour rappel, la syntaxe pour définir une fonction est la suivante:\n",
        "\n",
        "```python\n",
        "def nom_de_fonction(x):\n",
        "    \"\"\"\n",
        "    documentation\n",
        "    \"\"\"\n",
        "    # opérations python\n",
        "    x = x*x  \n",
        "    return x\n",
        "```\n",
        "\n",
        "Deux petites choses à ce sujet:\n",
        "- Les *paramètres* (ici `x`), définis après le nom de la fonction entre parenthèses, sont un ensemble de valeurs données en entrée à la fonction. Quand on active la fonction, on peut donc la faire s'exécuter avec des valeurs spécifiques.\n",
        "- Le `return` est la sortie de la fonction et permet de \"retourner\" une valeur: à la fin de l'exécution d'une fonction, Python supprime toutes les variables créées pendant l'exécution. Comme en maths, les valeurs retournées sont le \"résultat\" de la fonction, et seront donc utilisables en dehors de la fonction.\n",
        "\n",
        "Pour appeler une fonction, on fait:\n",
        "\n",
        "```python\n",
        "nom_de_fonction(2)  # les `()` indiquent que l'on appelle la fonction. les valeurs entre parenthèses sont des paramètres: on calculera ici le carré de 2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf33ff3",
      "metadata": {
        "id": "2cf33ff3"
      },
      "outputs": [],
      "source": [
        "# il y a une tripotées de syntaxes peu claires ici mais ce n'est pas un problème central\n",
        "def xmlid(text):\n",
        "    \"\"\"\n",
        "    fonction pour créer un @xml:id à partir de la chaîne de caractère `text`.\n",
        "    permet de normaliser la création d'identifiants uniques. on ne garde que\n",
        "    les caractères alphanumériques sans majuscules de `text` + on enlève les\n",
        "    accents des lettres avec `unidecode`.\n",
        "    \n",
        "    :param text: le texte à partir duquel produire un identifiant\n",
        "    :returns: l'identifiant\n",
        "    \"\"\"\n",
        "    return unidecode.unidecode(\"\".join( c for c in text.lower() if c.isalnum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b492810b",
      "metadata": {
        "id": "b492810b"
      },
      "source": [
        "---\n",
        "\n",
        "## Enrichissements automatiques \n",
        "\n",
        "### Les données géographiques\n",
        "\n",
        "Ici, on va faire 3 opérations:\n",
        "- extraire de tout le corpus l'ensemble des **lieux d'expédition/destination des lettres**. Ensuite, attribuer à chaque lieu un @xml:id avec la fonction définie ci-dessus\n",
        "- à partir du nom des lieux, lancer des **requêtes sur l'API [nominatim](https://nominatim.org/release-docs/develop/api/Search/)** pour géocoder nos lieux d'expédition/réception.\n",
        "- **créer un `settingDesc`**, un élément TEI dans le `teiHeader` qui permette de décrire les lieux auxquels sont liés un corpus et de stocker leur géolocalisation.\n",
        "\n",
        "#### 1. Définition des variables de base\n",
        "\n",
        "À la fin de cette étape, on aura donc un corpus géolocalisé, et dont le géocodage sera documenté par les documents XML eux-mêmes! On commence par définir nos variavbles de base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9692085",
      "metadata": {
        "id": "a9692085"
      },
      "outputs": [],
      "source": [
        "place_list = []  # liste de lieux\n",
        "endpoint = \"https://nominatim.openstreetmap.org/search?\"  # url de l'api nominatim    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64e2aed8",
      "metadata": {
        "id": "64e2aed8"
      },
      "source": [
        "C'est maintenant l'heure de notre première rencontre de LXML, la librairie pour parser et manipuler des fichiers XML. Ce n'est pas la librairie la plus simple à prendre en main (de loin), mais elle est très pratique pour de gros corpus.\n",
        "- [*parser*](https://en.wikipedia.org/wiki/Parsing), en informatique, c'est faire lire un fichier à un ordinateur de manière à ce qu'il comprenne et valide sa structure. Parser du XML, c'est donc rendre sa structure *machine readable*. L'ordinateur pourra faire la distinction entre les éléments, comprendre leur imbrication...\n",
        "\n",
        "On crée deux variables qui stockent chacune un objet `lxml` décrivant un élément XML (balise+texte+imbrication+attributs...).\n",
        "- la fonction `etree.Element()` crée un élément, la fonction `etree.SubElement()` crée un sous-élément d'un élément prééxistant\n",
        "- l'argument `nsmap` permet de définir un espace de nom, une des petites complexités du XML: une sémantique spécifique (comme la TEI) doit être définie par un nom et rattachée à une URI (équivalent de l'URL). Par exemple, l'espace de nom TEI est associé à \"http://www.tei-c.org/ns/1.0\". Cela permet de localiser les vocabulaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e249541b",
      "metadata": {
        "id": "e249541b"
      },
      "outputs": [],
      "source": [
        "settingDesc = etree.Element(\n",
        "    \"settingDesc\"   # nom de l'élément\n",
        "    , nsmap=NS_TEI  # l'espace de noms auquel il appartient\n",
        ")\n",
        "listPlace = etree.SubElement(\n",
        "    settingDesc      # l'élément parent, soit le `settingDesc` créé juste au dessus\n",
        "    , \"listPlace\"    # le nom de l'élément\n",
        "    , nsmap=NS_TEI   # son espace de nom\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6d6b7",
      "metadata": {
        "id": "70e6d6b7"
      },
      "source": [
        "#### 2. Création d'une liste de lieux\n",
        "\n",
        "On fouille notre corpus une première fois pour **construire une liste dédoublonnée de lieux d'expédition/réception de lettres**:\n",
        "- on lit tous les fichiers vers lesquels pointe notre liste `CORPUS`\n",
        "- on les parse avec `etree.parse(nom_de_fichier)`. Un fichier XML parsé est dit un *arbre*, vu sa structure arborescente\n",
        "- on extrait les noms de lieux avec `etree.xpath(expression_xpath)` (on verra ça plus bas)\n",
        "- on enlève certains caractères\n",
        "- on construit enfin `place_list`: à chaque itération, si une des `place` d'expédition/destination n'est pas présente dans la liste, on l'y ajoute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23686ea6",
      "metadata": {
        "id": "23686ea6"
      },
      "outputs": [],
      "source": [
        "for fpath in CORPUS:  # on accède à chaque chemin vers un fichier xml\n",
        "    tree = etree.parse(fpath, parser=PARSER)  # on parse chaque fichier\n",
        "    for place in tree.xpath(\".//tei:correspAction/tei:placeName\", namespaces=NS_TEI):  # on cible tous les lieux\n",
        "        place = place.text.replace(\"?\", \"\").strip()  # simplifier la chaîne de caractères\n",
        "        if (\n",
        "            place not in place_list \n",
        "            and not re.search(\"^(inconnu|aucun)$\", place)\n",
        "        ):\n",
        "            # ajouter le lieu s'il n'est pas déjà dans la liste\n",
        "            # en supprimant les notations équivalentes à \"NA\" (lieu inconnu)\n",
        "            place_list.append(place)\n",
        "    \n",
        "print(place_list)  # on montre notre liste de lieux"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13269c4c",
      "metadata": {
        "id": "13269c4c"
      },
      "source": [
        "Il s'est passé quoi?? Plus précisément, c'est quoi cette ligne??\n",
        "\n",
        "```python\n",
        "tree.xpath(\".//tei:correspAction/tei:placeName\", namespaces=NS_TEI)\n",
        "```\n",
        "\n",
        "`tree.xpath` permet **d'évaluer une expression `xPath` sur un arbre XML**. `xPath` est un langage assez compact et assez puissant pour naviguer à l'intérieur d'un document XML. Ce langage permet de traverser un document complexe, en sautant les éléments intermédiaires pour cibler ceux qui nous intéressent seulement. Parmi les principes:\n",
        "- une `xPath` est structurée de façon analogue à un chemin de fichier ou à une URL: `elementA/elementB/elementC`...\n",
        "- une xpath permet de cibler des *nœuds* à l'intérieur du document. Ces nœuds peuvent être:\n",
        "    - des éléments XML, écrits sans préfixe `nom-de-l'élément` \n",
        "    - un attribut, préfixé une `@`: `@ref`\n",
        "    - `*` représente \"n'importe quel nœud\"\n",
        "    - `.` représente le nœud actuel\n",
        "    - une fonction qui permet d'évaluer un résultat\n",
        "- dans une xpath, des nœuds peuvent être ciblés en fonction de:\n",
        "    - leur positionnement relatif: \n",
        "        - `nœudA/nœudB` permet de cibler un nœud B qui est l'enfant direct d'un nœud A.\n",
        "        - `nœudA//nœudB` permet de cibler un nœud B qui est descendant du nœud A: il est inclus dans celui-ci, mais il peut y avoir plusieurs intermédiaires entre A et B.\n",
        "        - il est aussi possible de traverser l'arbre dans des directions différentes: de l'enfant au parent...\n",
        "    - des propriétés, écrites entre `[]`. \n",
        "        - Par exemple, `a[@xml:id=\"Artemisia\"]` permet de cibler un élément `a` qui a un attribut `@xml:id` dont la valeur est `Artemisia`. \n",
        "        - on peut mettre bout à bout les propriétés: `*[@xml:id=\"Artemisia\"][type=\"painter\"]` permet de cibler n'importe quel élément dont l'`@xml:id` est \"Artemisia\" et le `@type` est \"painter\". \n",
        "- on peut bien sûr combiner propriétés, sélection d'éléments et d'attributs, ajouter des fonctions et même mélanger les directions dans lesquelles l'arbre est traversé pour arriver à des très structures complexes:\n",
        "    ```\n",
        "    body//rdg[not(ancestor::rdgGrp)][not(ancestor::app//app)]\n",
        "    ```\n",
        "        \n",
        "En bref, `.//tei:correspAction/tei:placeName` permet de cibler tous les `placeName` dans un `correspAction`. Ces éléments doivent être descendants de l'élément actuel.\n",
        "\n",
        "[Pour aller plus loin sur les xpath : )](https://github.com/paulhectork/tnah2021_cours/blob/main/export/cours_markdown/xquery-xpath_fiche.md), descendre jusqu'à arriver dans la partie `XPATH`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a5d3f60",
      "metadata": {
        "id": "6a5d3f60"
      },
      "source": [
        "#### 3. Géocodage des lieux\n",
        "\n",
        "On a créé une liste de lieux où les lettres sont écrites ou envoyées. On peut maintenant **géocoder les lieux**, c'est-à-dire, à partir d'une addresse \"lisible par les humain.e.s\", obtenir des coordonnées. Pour cela, on utilise l'API Nominatim, qui permet d'accéder aux données d'Openstreetmap.\n",
        "\n",
        "Mais d'abord: **une API, c'est quoi?** La manière la plus brève de définir une API Web, c'est de dire que c'est **un site internet pour machines**. Une API permet à une machine d'interagir avec un serveur à distance, de lui envoyer et de récupérer des données brutes, de façon automatique. \n",
        "\n",
        "Dans notre cas, l'API sera utilisée pour **faire une recherche en plein texte à notre place**, pour obtenir des coordonnées géographiques à partir d'un nom de ville. Plutôt que de faire la recherche à la main, on fait un script qui **effectue la recherche à notre place, stocke les résultats et met à jour nos documents XML.**\n",
        "\n",
        "Pour que notre script communique avec uns serveur à distance, les APIs utilisent les mêmes standards que n'importe quels sites Web, et surtout le **HTTP(S)**. Ce standard permet:\n",
        "- à un **client** (notre script) de poser une question sous la forme d'une URL à un serveur.\n",
        "- à un **serveur** (Openstreetmap) de répondre en nous renvoyant des données correspondant à notre requête.\n",
        "\n",
        "![client server architecture](https://miro.medium.com/v2/resize:fit:1400/0*9iZ6PlYHEOwi0-X-)\n",
        "\n",
        "Pour faire nos requêtes, on utilise **la librairie `Request`.** Pour chaque lieu, \n",
        "- on construit un élément HTML `place` documentant le lieu\n",
        "- on construit une URL pour faire une requête sur cette ville\n",
        "- on lance la requête, récupère et sauvegarde le GeoJSON produit, et on ajoute les géocoordonnées au `place`.\n",
        "\n",
        "[Pour plus d'infos sur les APIs : )](https://github.com/paulhectork/cours_ens2023_fouille_de_texte/blob/main/2_bonus_creation_corpus.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e93ee2e",
      "metadata": {
        "id": "0e93ee2e"
      },
      "outputs": [],
      "source": [
        "for placename in place_list:\n",
        "    # créer un identifiant unique @xml:id\n",
        "    idx = xmlid(placename)\n",
        "        \n",
        "    # créer l'élément tei `place`, contenu par `listPlace`\n",
        "    # et qui contient toutes les informations sur le lieu\n",
        "    place = etree.SubElement(\n",
        "        listPlace\n",
        "        , \"place\"\n",
        "        , nsmap=NS_TEI\n",
        "    )\n",
        "    # définir son @xml:id\n",
        "    place.set(\"{http://www.w3.org/XML/1998/namespace}id\", idx)\n",
        "    # y ajouter un sous élément `placeName` contenant le nom du lieu.\n",
        "    etree.SubElement(\n",
        "        place\n",
        "        , \"placeName\"\n",
        "        , nsmap=NS_TEI\n",
        "    ).text = placename\n",
        "    \n",
        "    # jusque là tout va bien. on va maintenant commencer à faire des requêtes:\n",
        "    if placename == \"NA\" or placename == \"na\":\n",
        "        # continue stoppe ici l'itération actuelle \n",
        "        # => la requête API n'est pas lancée et on passe à l'item suivant\n",
        "        continue\n",
        "    \n",
        "    time.sleep(1)  # il faut attendre 1s entre 2 requêtes\n",
        "    \n",
        "    # requests.get() lance une requête HTTP Get. on lui donne 2 arguments:\n",
        "    # - `endpoint`: l'URL de pase de l'API\n",
        "    # - `params`: un dictionnaire de paramètres. request construit une URL\n",
        "    #    en combinant l'endpoint et les params.\n",
        "    r = requests.get(endpoint, params={ \n",
        "        \"city\": placename,    # le nom de la ville recherchée\n",
        "        \"format\": \"geojson\",  # format de la réponse: json\n",
        "        \"limit\": 1            # nombre de résultats à afficher\n",
        "    })\n",
        "    \n",
        "    # si on ne trouve pas de nom de ville, alors on fait une recherche \n",
        "    # libre en utilisant le param `q` à la place de `city`\n",
        "    if len(r.json()) == 0:\n",
        "        time.sleep(1)\n",
        "        r = requests.get(endpoint, params={ \n",
        "            \"q\": placename,       # `q`: query, paramètre de recherche libre\n",
        "            \"format\": \"geojson\",  # format de la réponse: geojson\n",
        "            \"limit\": 1            # nombre de résultats à afficher\n",
        "        })\n",
        "        \n",
        "    # les requêtes ont été lancées. \n",
        "    # si la requête s'est bien passée et qu'on a des résultats, alors\n",
        "    # - on parse le geojson retourné pour extraire les informations\n",
        "    # - on complète notre élément xml `place` avec un `location` qui\n",
        "    #   contient un `geo`, celui-ci contenant les géocoordonnées\n",
        "    # - on enregristre notre geojson dans le dossier `WEB/json/`\n",
        "    print(r.url)    # on affiche l'url; une API fonctionne bien comme un site normal!\n",
        "    res = r.json()  \n",
        "    if r.status_code == 200 and len(res[\"features\"]) > 0:  # si il n'y a pas d'erreur\n",
        "            \n",
        "        # on extrait les coordonnées et les ajoute à `place`\n",
        "        if res[\"features\"][0][\"geometry\"][\"type\"] == \"Point\":\n",
        "            # on convertit la liste de coordonnées en string\n",
        "            coordinates = \"\".join( \n",
        "                f\"{c} \" for c in res[\"features\"][0][\"geometry\"][\"coordinates\"] \n",
        "            ).strip()\n",
        "            # on crée notre élément xml\n",
        "            location = etree.SubElement(\n",
        "                place\n",
        "                , \"location\"\n",
        "                , nsmap=NS_TEI\n",
        "            )\n",
        "            etree.SubElement(\n",
        "                location\n",
        "                , \"geo\"\n",
        "                , nsmap=NS_TEI\n",
        "            ).text = coordinates\n",
        "                \n",
        "            # enfin, on enregistre notre json. la réponse est en geojson => on crée\n",
        "            # un fichier geojson et on l'enregistre, pour pouvoir y accéder +tard\n",
        "            with open( os.path.join(WEB, \"json\", f\"{idx}.geojson\"), mode=\"w\" ) as fh:\n",
        "                json.dump(res, fh, indent=4)\n",
        "                \n",
        "        else:\n",
        "            # il n'y a pas de coordonnées, c'est étrange => on print\n",
        "            print(f\"pas de coordonnées pour '{placename}'\", \"\\n\", res, \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de6a055d",
      "metadata": {
        "id": "de6a055d"
      },
      "source": [
        "**Dans le bloc de code précédent**, on a géocodé toutes les villes du corpus, construit notre `listPlace` et on a produit un paquet de fichiers `geojson` qui seront utiles pour notre visualisation. En faisant `print(r.url)`, on voit bien qu'une API fonctionne comme un site normal: on pose une question à un serveur, le serveur renvoie une réponse. La seule chose qui change, c'est le format.\n",
        "\n",
        "#### 4. Mise à jour des fichiers XML\n",
        "\n",
        "Toutes nos données sont prêtes. Il ne reste donc qu'à mettre à jour notre corpus de fichiers XML:\n",
        "- on ajoute au `profileDesc` le `settingDesc` créé plus haut. \n",
        "- on met à jour le `correspAction` (qui décrit la correspondance): on ajoute un attribut `@ref` aux `correspAction/placeName` pour faire un renvoi entre cette information (envoi de la lettre spécifique) et le `listPlace`, qui centralise toutes nos informations spatiales.\n",
        "\n",
        "Tout ça va être l'occasion de voir des xpath un peu plus complexes c:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c10ff34",
      "metadata": {
        "id": "3c10ff34"
      },
      "outputs": [],
      "source": [
        "# on a pris l'habitude: on lit tous nos fichiers XML et on les parse\n",
        "for fpath in CORPUS:\n",
        "    tree = etree.parse(fpath, parser=PARSER)\n",
        "    \n",
        "    # ajout du `settingDesc`\n",
        "    tree.xpath(\".//tei:profileDesc\", namespaces=NS_TEI)[0].append(settingDesc)\n",
        "    \n",
        "    # ajout des `@ref` aux `placeName`.\n",
        "    # pour ce faire, on reconstruit l'`@xml:id` à partir de son orthographe => on peut\n",
        "    # comparer entre l'identifiant créé ici (`placetext`) et l'`@xml:id` du `settingDesc`   \n",
        "    for placename in tree.xpath(\".//*[not(tei:settingDesc)]//tei:placeName\", namespaces=NS_TEI):\n",
        "        placetext = xmlid(placename.text)  # on reconstruit l'@xml:id\n",
        "        \n",
        "        # inconnu, aucun => on donne la valeur `#na` à `@ref`\n",
        "        if re.search(\"^(inconnu|aucun)$\", placetext):\n",
        "            placename.set(\"ref\", \"#na\")\n",
        "\n",
        "        # pour tous les autres cas, on définit le `@ref`.\n",
        "        # celui-ci est toujours préfixé d'un `#` pour montrer\n",
        "        # qu'on fait référence à un @xml:id\n",
        "        else:\n",
        "            placename.set(\"ref\", f\"#{placetext}\")\n",
        "                \n",
        "    # enfin, on écrit les arbres xml mis à jour dans les bons fichiers\n",
        "    etree.cleanup_namespaces(tree)\n",
        "    etree.ElementTree(tree.getroot()).write(\n",
        "        fpath\n",
        "        , pretty_print=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c91c3f9",
      "metadata": {
        "id": "7c91c3f9"
      },
      "source": [
        "---\n",
        "\n",
        "### Les entités nommées\n",
        "\n",
        "Avant de passer aux visualisations, il est temps de travailler sur certaines autres entités nommées de notre corpus: **les destinataires et expéditeur.ice.s des lettres**. Il va s'agir de \n",
        "- les identifier\n",
        "- les dédoublonner \n",
        "- les classifier\n",
        "- pour finir, créer un `particDesc` qui contienne toutes ces infos. \n",
        "\n",
        "Si l'identification, le dédoublonnage et la création du `particDesc` sont assez semblables à l'étape précédente, la classification automatique du type d'expéditeur.ice / destinataire va nous introduire à **l'apprentissage machine** et au **traitement automatisé des langues**.\n",
        "\n",
        "Mais avant tout: **une entité nommée, c'est quoi?** C'est une [expression linguistique référentielle](https://fr.wikipedia.org/wiki/Entit%C3%A9_nomm%C3%A9e), soit quelque chose, qui dans le langage, fait référence à quelque chose. Notion très vague: personne, organisation, œuvre d'art ou tout autre objet du réel sont des entitées nommées.\n",
        "\n",
        "#### Chaîne de traitement\n",
        "\n",
        "- on créé `entities`, un dictionnaire associant à l'identifiant \n",
        "  unique de chaque expéditeur/destinataire les différentes orthographes \n",
        "  de son nom + la langue dans laquelle la lettre comportant son nom\n",
        "  est écrite\n",
        "- avec Spacy, on identifie le type de chaque entité nommée: est-ce que\n",
        "  c'est le nom d'une personne, d'une organisation? si oui, quel type \n",
        "  d'organisation?\n",
        "- on crée un `particDesc` qui contient un `listPerson` avec la liste\n",
        "  de `person` et un ;`listOrg` avec la liste d'`org` expéditrices/destinataires \n",
        "  de lettres dans le corpus, en fonction des données produites par spacy\n",
        "- on met à jour le `correspDesc` en fonction des informations dans\n",
        "  le `particDesc`\n",
        "- on met à jour les fichiers\n",
        "\n",
        "\n",
        "#### 1. Définition des éléments de base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "954703e1",
      "metadata": {
        "id": "954703e1"
      },
      "outputs": [],
      "source": [
        "# entities associe à un xml:id 3 clés:\n",
        "# - `name`: la liste des différentes orthographes de ce nom\n",
        "# - `type`: le type d'entité (personne, organisation...)\n",
        "# - `lang`: la langue du nom (pour définir le modèle spacy à utiliser)\n",
        "entities = {}\n",
        "lang_dict = {}                          # dictionnaire associant à l'xml:id d'une lettre la langue parlée                       \n",
        "nlp_fr = spacy.load(\"en_core_web_trf\")  # modèle spacy pour l'anglais\n",
        "nlp_en = spacy.load(\"fr_dep_news_trf\")  # modèle spacy pour le français\n",
        "particDesc = etree.Element(             # élément xml qui accueillera la liste de personnes \n",
        "    \"particDesc\"\n",
        "    , nsmap=NS_TEI\n",
        ")\n",
        "listPerson = etree.SubElement(          # la liste de personnes\n",
        "    particDesc\n",
        "    , \"listPerson\"\n",
        "    , nsmap=NS_TEI\n",
        ")\n",
        "listOrg = etree.SubElement(             # la liste des organisations\n",
        "    particDesc\n",
        "    , \"listOrg\"\n",
        "    , nsmap=NS_TEI\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95076278",
      "metadata": {
        "id": "95076278"
      },
      "source": [
        "#### 2. Le premier traitement du corpus\n",
        "\n",
        "On parse le premier corpus une première fois pour:\n",
        "- créer un **`listPerson`**, qui fonctionne de la même manière que le `listPlace` créé au dessus\n",
        "- on **détermine la langue** de chaque lettre, pour choisir le modèle spacy à utiliser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36d6458a",
      "metadata": {
        "id": "36d6458a"
      },
      "outputs": [],
      "source": [
        "for fpath in CORPUS:\n",
        "    tree = etree.parse(fpath, parser=PARSER)\n",
        "    letter_idx = tree.xpath(\"./@xml:id\", namespaces=NS_TEI)[0]  # letter's @xml:id\n",
        "        \n",
        "    # on détecte la langue de chaque lettre. pour ce faire, on extrait\n",
        "    # le texte de la lettre avec une xpath, on en fait une seule chaîne\n",
        "    # de caractère et on utilise `detect()` de la librairie `langdetect`\n",
        "    lang = detect( \" \".join(s for s in tree.xpath(\".//tei:body//text()\", namespaces=NS_TEI)) )\n",
        "    lang_dict[letter_idx] = lang  # on ajoute la langue à `lang_dict`\n",
        "    \n",
        "    # ensuite, on construit le `listPerson`\n",
        "    for pers in tree.xpath(\".//tei:correspAction//tei:persName\", namespaces=NS_TEI):\n",
        "            \n",
        "        # pour éviter les doublons inutiles, on ajoute pas tous les noms à `place_dict`\n",
        "        # - on enlève les initiales\n",
        "        # - on simplifie les chaînes de caractères avec la fonction `xmlid()`\n",
        "        # - on ajoute pas de doublons\n",
        "        # - à chaque fois, on ajoute le code de la langue pour déterminer le modèle\n",
        "        #   à utiliser sur les noms extraits\n",
        "        \n",
        "        idx = xmlid(re.sub(\"((?<=\\s)|(?<=^))[A-Z][a-zàâäéèûüùîïì]*\\.\", \"\", pers.text))  # enlever initiales\n",
        "            \n",
        "        # traitement spécifique pour `na` et équivalents: l'@xml:id défini sera `napartic`\n",
        "        if re.search(\"^(inconnu|aucun|na)$\", idx):\n",
        "            if \"napartic\" not in entities.keys():\n",
        "                entities[\"napartic\"] = { \"name\": [pers.text], \"type\": \"\", \"lang\": [lang] }\n",
        "            elif pers.text not in entities[\"napartic\"][\"name\"]:\n",
        "                entities[\"napartic\"][\"name\"].append(pers.text)\n",
        "                entities[\"napartic\"][\"lang\"].append(lang)\n",
        "        # pour les autres noms:\n",
        "        else:\n",
        "            if idx not in entities.keys():\n",
        "                entities[idx] = { \"name\": [pers.text], \"type\": \"\", \"lang\": [lang] }\n",
        "            elif  pers.text not in entities[idx][\"name\"]:\n",
        "                entities[idx][\"name\"].append(pers.text)\n",
        "                entities[idx][\"lang\"].append(lang)\n",
        "\n",
        "print(lang_dict)\n",
        "print(entities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2587b842",
      "metadata": {
        "id": "2587b842"
      },
      "source": [
        "#### 3. Détecter le type d'entité dans `entities`\n",
        "\n",
        "Le dictionnaire `entities` ne contient que des entités nommées. Mais elles peuvent être de types assez différents. Pour limiter le bruit, et mieux comprendre qui écrit à qui, nous allons donc utiliser la reconnaissance d'entités nommées de Spacy, non pas identifier des entités, mais pour classifier celles-ci."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae38781",
      "metadata": {
        "scrolled": false,
        "id": "8ae38781"
      },
      "outputs": [],
      "source": [
        "for k in entities.keys():\n",
        "    \n",
        "    # on extrait le mode, soit la langue la plus r\n",
        "    # épandue pour la liste de lettres avec ce nom\n",
        "    if len(entities[k][\"lang\"]) > 0:\n",
        "        entities[k][\"lang\"] = mode(entities[k][\"lang\"])\n",
        "    else:\n",
        "        entities[k][\"lang\"] = \"\"\n",
        "    \n",
        "    # on traite tous les noms associés à un @xml:id\n",
        "    for name in entities[k][\"name\"]:\n",
        "        \n",
        "        # on sélectionne le bon modèle en fonction du langage. \n",
        "        # si le langage détecté n'est ni le français, ni l'anglais,\n",
        "        # on affiche une erreur mais on utilise le modèle français: la\n",
        "        # lettre est surement en français \n",
        "        if entities[k][\"lang\"] == \"fr\":\n",
        "            doc = nlp_fr(name)\n",
        "        elif entities[k][\"lang\"] == \"en\":\n",
        "            doc = nlp_en(name)\n",
        "        else:\n",
        "            print(f\"pas de modèle disponible pour la langue '{entities[k]['lang']}'\"\n",
        "                  + f\"détectée dans la lettre: '{letter_idx}'. utilisation du modèle\"\n",
        "                  + \" français par défaut 'fr_dep_news_trf'\")\n",
        "            doc = nlp_fr(name)\n",
        "        \n",
        "        # pour avoir une idée de la classification produite par spacy\n",
        "        for ent in doc.ents: \n",
        "            print(\n",
        "                f\"source ~ {name}\\n\"\n",
        "                , f\"détecté ~ {ent.text}\\n\"\n",
        "                , f\"type ~ {ent.label_} {spacy.explain(ent.label_)}\"\n",
        "                , \"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\"\n",
        "            )\n",
        "\n",
        "        # enfin, avec spacy, on détecte le type d'entité nommée encodée dans\n",
        "        # les `correspAction` et on ajoute le label donné à l'entité nommée par spacy \n",
        "        # dans `name` à `entities`. spacy produit plusieurs entités nommées => on extrait\n",
        "        # le mode, soit le type d'entité le + souvent détecté dans notre `name`. vu la \n",
        "        # taille de notre corpus c'est un peu inutile, mais bon\n",
        "        if len(list(doc.ents)) > 0:\n",
        "            entities[k][\"type\"] = mode([ ent.label_ for ent in doc.ents ])  # on calcule le mode parmi tous les types d'entités nommées relevées\n",
        "        else:\n",
        "            entities[k][\"type\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceaaeda1",
      "metadata": {
        "id": "ceaaeda1"
      },
      "source": [
        "#### 4. Créer le `particDesc`\n",
        "\n",
        "On en a fini avec Spacy! La librairie est beaucoup, beaucoup plus puissante que ce pourquoi on l'a utilisée, mais elle a quand même été utile ici: sur un corpus de plus de 100 lettres avec un grand nombre d'auteur.ice.s, il devient difficile d'utiliser seulement de la détection de motifs (chercher \"à la main\", ou avec des expressions régulières, des termes qui permettent de déterminer si un nom est celui d'une personne ou d'une organisation). On bénéficie avec Spacy d'une énorme base de données de vocabulaire (pleine de biais, bien sûr) qui permet de faire un tri plus fin.\n",
        "\n",
        "Il s'agit maintenant **d'encoder cette classification en XML-TEI**. Sur le principe, c'est comme la création du `settingDesc`, mais on va faire un peu plus complexe: \n",
        "- le `particDesc` aura 2 enfants:\n",
        "    - un `listPerson` qui contienne toutes les entités détectées comme des personnes\n",
        "    - un `listOrg`, qui contient toutes les entités détectées comme autre chose que des personnes\n",
        "- chaque `person` ou `org` aura un attribut `@type` qui contienne le type d'entité détecté par Spacy.\n",
        "- comme pour les lieux, on va ensuite mettre à jour le `correspAction` pour qu'il contienne un `@ref` aux `@xml:id` des entités dans le `particDesc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ef8270",
      "metadata": {
        "id": "54ef8270"
      },
      "outputs": [],
      "source": [
        "for k in entities.keys():\n",
        "    # déterminer le type d'élément à créer en fonction du type d'entité\n",
        "    if entities[k][\"type\"] == \"PERSON\" or entities[k][\"type\"] == \"\":\n",
        "        parent = listPerson  # l'élément parent: listPerson|listOrg\n",
        "        el = \"person\"        # le nom de l'élément lui-même\n",
        "        elname = \"persName\"  # si il doit contenir un `persName` ou un `orgName`\n",
        "    else:\n",
        "        parent = listOrg\n",
        "        el = \"org\"\n",
        "        elname = \"orgName\"\n",
        "    # définir `entity`\n",
        "    entity = etree.SubElement(\n",
        "        parent\n",
        "        , el\n",
        "        , nsmap=NS_TEI\n",
        "    )\n",
        "    \n",
        "    # définir les attributs\n",
        "    if entities[k][\"type\"] != \"\":\n",
        "        entity.set(\"type\", entities[k][\"type\"])  # définir le `@type`: type d'entité\n",
        "    entity.set(\"{http://www.w3.org/XML/1998/namespace}id\", k)  # définir l'identifiant de `entity`\n",
        "    \n",
        "    # définir ses sous-éléments `persName` ou `orgName`, portant les \n",
        "    # variantes d'appellations associées à cette entité dans les \n",
        "    # différentes lettres du corpus\n",
        "    for name in entities[k][\"name\"]:\n",
        "        etree.SubElement(\n",
        "            entity\n",
        "            , elname\n",
        "            , nsmap=NS_TEI\n",
        "        ).text = name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6960f043",
      "metadata": {
        "id": "6960f043"
      },
      "source": [
        "#### 5. Mettre à jour les fichiers XML\n",
        "\n",
        "On reparse (encore et toujours) les fichiers XML pour:\n",
        "- modifier les `correspAction`: si le `persName` renvoie en fait à une organisation, alors remplacer ce `persName` par un `orgName` (on fait ça à l'aide des différents `persName` / `orgName` dans le `particDesc`)\n",
        "- ajouter les `@ref` dans le `correspAction` qui font référence aux @xml:id des entités identifiées dans le `particDesc`\n",
        "- ajouter le `particDesc` au `encodingDesc` du fichier xml\n",
        "- sauvegarder les arbres mis à jour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7692c222",
      "metadata": {
        "id": "7692c222"
      },
      "outputs": [],
      "source": [
        "for fpath in CORPUS:\n",
        "    tree = etree.parse(fpath, parser=PARSER)\n",
        "        \n",
        "    # ajout du `particDesc` à l'arbre\n",
        "    tree.xpath(\".//tei:profileDesc\", namespaces=NS_TEI)[0].append(particDesc)\n",
        "        \n",
        "    # on modifie les correspAction:\n",
        "    # `corresp` = tous les noms expéditeur.ice.s/destinataires dans le `correspAction`\n",
        "    for corresp in tree.xpath(\".//tei:correspAction/tei:persName\", namespaces=NS_TEI):\n",
        "            \n",
        "        # on cible l'élément du `particDesc` qui correspond à `corresp`\n",
        "        for matched_entity in particDesc.xpath(\n",
        "            f\".//*[./text()='{corresp.text}']\"\n",
        "            , namespaces=NS_TEI\n",
        "        ):\n",
        "            # on met à jour le tag de `corresp`: si avec spacy, on a détecté que le nom\n",
        "            # dans `corresp` n'est pas celui d'une personne, alors on change `persName` en `orgName`\n",
        "            corresp.tag = matched_entity.tag\n",
        "            # enfin, on ajoute un `@ref` à `corresp` qui pointe vers le bon\n",
        "            # `persName / org` dans le `particDesc` \n",
        "            idx = matched_entity.xpath(\"./parent::*/@xml:id\")[0]\n",
        "            corresp.set(\"ref\", f\"#{idx}\")\n",
        "        \n",
        "    # on sauvegarde le fichier\n",
        "    etree.cleanup_namespaces(tree)\n",
        "    etree.ElementTree(tree.getroot()).write(\n",
        "        fpath\n",
        "        , pretty_print=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4528aab0",
      "metadata": {
        "id": "4528aab0"
      },
      "source": [
        "## Visualisation\n",
        "\n",
        "Jusqu'ici, on a déjà beaucoup enrichi notre corpus:\n",
        "- index des lieux\n",
        "- géocodage des lieux\n",
        "- index des expéditeur.ice.s et destinataires, avec classification automatique\n",
        "- enrichissement des `correspAction` pour créer des liens avec les indexes.\n",
        "\n",
        "C'est ici qu'on commence à voir l'utilité de la TEI (enfin, j'espère): tous ces enrichissements peuvent être ajoutés après l'encodage du texte et sont encodés à l'intérieur même du document qui contient le texte. On peut donc développer des **chaînes éditoriales** assez complexes.\n",
        "\n",
        "Pour finir, on va chercher à **produire des visualisations** à partir de notre corpus:\n",
        "- une visualisation en réseau du corpus: qui écrit à qui? en quel volume?\n",
        "- une visualisation cartographique: de quelles villes les lettres sont-elles expédiées? Vers quelles villes?\n",
        "\n",
        "Dans les deux cas, **l'approche va être semblable**:\n",
        "- parser les fichiers XML pour extraire les informations utiles\n",
        "- produire des graphes des lieux ou personnes et des relations entre elles.eux\n",
        "- utiliser ces graphes pour construire des cartes/réseaux.\n",
        "\n",
        "### Analyse de réseau\n",
        "\n",
        "Pour faire l'analyse de réseaux, on utilise la librairie Python Pyvis qui produit des graphes interactifs. Le réseau représente les relations entre expéditeur.ice.s et destinataires au sein du corpus Matsutaka. Ce réseau aura les caractéristiques suivantes:\n",
        "- c'est un graphe orienté (une relation de A vers B =/= une relation de B vers A)\n",
        "- les nœuds sont les expéditeur.ice.s et destinataires du corpus\n",
        "- les arrêtes sont les lettres échangées entre elles et eux\n",
        "- la taille des nœuds et l'épaisseur des arrêtes est déterminée par le volume de lettres\n",
        "\n",
        "#### 1. Extraction de données\n",
        "\n",
        "On représente nos nœuds de la façon suivante:\n",
        "\n",
        "```python\n",
        "{\n",
        "    # 1ere entrée\n",
        "    \"@xml:id de la personne\": [\n",
        "        \"nom complet qui sera affiché\"\n",
        "        , <nombre de mentions comme expéditeur.ice ou destinataire>\n",
        "    ]\n",
        "    # 2e entrée\n",
        "    , \"@xml:id\": [\n",
        "        \"nom complet\"\n",
        "        , <décompte>\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "Et nos arrêtes comme ça:\n",
        "\n",
        "```python\n",
        "[\n",
        "    # 1ere relation\n",
        "    { \n",
        "        \"from\": \"@xml:id de l'expéditeur.ice\",\n",
        "        \"to\": \"@xml:id du destinataire\"\n",
        "        \"count\": <nombre de relations dans ce sens entre expéditeur.ice et destinataire>\n",
        "    }\n",
        "    # 2e relation\n",
        "    , {\n",
        "        \"from\": \"@xml:id\",\n",
        "        \"to\":\"@xml:id\",\n",
        "        \"count\": <décompte>\n",
        "    }\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8bb1f79",
      "metadata": {
        "scrolled": true,
        "id": "b8bb1f79"
      },
      "outputs": [],
      "source": [
        "# on lit toutes les lettres du corpus, extrait les données pour \n",
        "# construire un graphe (noms des expéditeur.ice.s/destinataire et \n",
        "# nombre de mention de chacun.e, relation orientées entre expéditeur.ice\n",
        "# et destinataire et nombre de relations orientées)\n",
        "nodes = {}  # les nœuds.\n",
        "edges = []  # les arrêtes\n",
        "for fpath in CORPUS:\n",
        "    tree = etree.parse(fpath, parser=PARSER)\n",
        "        \n",
        "    # @xml:id de l'expéditeurice et du/de la destinataire\n",
        "    sender = tree.xpath(\n",
        "        \"//tei:correspAction[@type='sent']/*[not(tei:placeName)][not(tei:date)]/@ref\"\n",
        "        , namespaces=NS_TEI\n",
        "    )[0].replace(\"#\", \"\")\n",
        "    receiver = tree.xpath(\n",
        "        \"//tei:correspAction[@type='received']/*[not(tei:placeName)][not(tei:date)]/@ref\"\n",
        "        , namespaces=NS_TEI\n",
        "    )[0].replace(\"#\", \"\")\n",
        "                \n",
        "    # en utilisant l'@xml:id, on prend le nom canonique du `particDesc`\n",
        "    sender_name = tree.xpath(\n",
        "        f\".//tei:particDesc//*[@xml:id='{sender}']/*\"\n",
        "        , namespaces=NS_TEI\n",
        "    )[0].text\n",
        "    receiver_name = tree.xpath(\n",
        "        f\".//tei:particDesc//*[@xml:id='{receiver}']/*\"\n",
        "        , namespaces=NS_TEI\n",
        "    )[0].text\n",
        "\n",
        "    # on ajoute `sender`/`receiver` à `nodes`. on ne distingue pas le rôle d'expéditeur/destinataire\n",
        "    if sender not in nodes.keys():\n",
        "        nodes[sender] = [ sender_name, 1 ]      # 1ere fois que `sender` est identifié comme nœud => créer une nv entrée\n",
        "    else:\n",
        "        nodes[sender][1] += 1                   # sinon, on incrémente le compteur d'occurrences pour cette entité\n",
        "    if receiver not in nodes.keys():\n",
        "        nodes[receiver] = [ receiver_name, 1 ]  # 1ere fois que `receiver` est identifié comme nœud => nv entrée\n",
        "    else:\n",
        "        nodes[receiver][1] +=1                  # sinon, on incrémente le compteur d'occurrences pour cette entité\n",
        "    \n",
        "    # on ajoute la relation entre `sender` & `receiver` à `edges`\n",
        "    # si la relation orientée expéditeurice->destinataire n'existe pas on l'ajoute\n",
        "    if not any( [sender, receiver] == [edge[\"from\"], edge[\"to\"]] for edge in edges ):\n",
        "        edges.append({ \"from\": sender, \"to\": receiver, \"count\": 1 })\n",
        "    # sinon, on récupère le dictionnaire dans `edges` qui décrit la bonne relation et on incrémente son compteur \n",
        "    else:\n",
        "        # on sélectionne l'index de la bonne relation\n",
        "        for edge in edges:\n",
        "            if [edge[\"from\"], edge[\"to\"]] == [sender, receiver]:\n",
        "                idx = edges.index(edge)\n",
        "        edges[idx][\"count\"] += 1  # on incrémente son compteur\n",
        "\n",
        "print(nodes)\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc05b92",
      "metadata": {
        "id": "3bc05b92"
      },
      "source": [
        "#### 2. Créer le réseau\n",
        "\n",
        "On crée un objet `network` de `pyvis` et in y ajoute les nœuds avec `add_node()` et les arrêtes avec `add_edges()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fb13d2",
      "metadata": {
        "scrolled": true,
        "id": "82fb13d2"
      },
      "outputs": [],
      "source": [
        "# créer le réseau\n",
        "ntw = Network( \n",
        "    directed=True                # on travaille avec un graphe orienté\n",
        "    , bgcolor=COLORS[\"gold\"]    # couleur d'arrière-plan\n",
        "    , font_color=COLORS[\"darkgreen\"]  # couleur de police\n",
        "    , filter_menu=True                # un menu pour filtrer par les propriétés des nœuds et arrêtes\n",
        "    , notebook=True\n",
        "    , cdn_resources=\"remote\"\n",
        ")\n",
        "\n",
        "# ajout des nœuds\n",
        "for k, v in nodes.items():\n",
        "    ntw.add_node(\n",
        "        k                       # l'identifiant du nœud: un @xml:id\n",
        "        , label=v[0]            # le nom affiché\n",
        "        , size=v[1]             # la taille du nœud, déterminée par le nb d'occurrences dans le corpus\n",
        "        , shape=\"dot\"           # la forme\n",
        "        , color=COLORS[\"plum\"]  # la couleur du nœud\n",
        "        , title=f\"{v[0]} participe à {v[1]} échanges dans le corpus.\"  # texte à afficher quand on clique s/ le nœud\n",
        "    )\n",
        "\n",
        "# ajout des arrêtes\n",
        "for edge in edges:\n",
        "    ntw.add_edge(\n",
        "        edge[\"from\"]           # identifiant du nœud représentant l'expéditeur.ice (défini dans `.add_nodes()`)\n",
        "        , edge[\"to\"]           # identifiant du nœud représentant le/la destinatairice\n",
        "        , width=edge[\"count\"]  # l'épaisseur de l'arrête dépend du nombre d'envois\n",
        "        , title=f\"{edge['count']} lettres de { nodes[edge['from']][0] } pour { nodes[edge['to']][0] }\"\n",
        "    )\n",
        "\n",
        "# bidouiller la physique    \n",
        "ntw.barnes_hut(overlap=1, gravity=-40000)  # la position des points du réseau)\n",
        "ntw.toggle_physics(True)  # conseillé par la doc\n",
        "\n",
        "# afficher le fichier. il y a un bug avec google colab => enregistrer network.html et afficher la page dans un navigateur\n",
        "ntw.show(\"network.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3d3728",
      "metadata": {
        "id": "3f3d3728"
      },
      "source": [
        "### Cartographie\n",
        "\n",
        "Notre cartographie du corpus fonctionne sur d'une manière similaire à la visualisation en graphe: il s'agit de placer des points sur une carte (les nœuds) et ensuite de créer des relations entre eux (les arrêtes). La carte représente les villes d'expédition/destination des lettres du corpus, et les relations entre ces villes. Notre carte aura les caractéristiques suivantes:\n",
        "- les relations entre villes sont non-orientées (une relation de A à B == une relation de B à A)\n",
        "- la taille des marqueurs positionnés sur les villes est déterminée par le nombre de lettres qui y sont liées\n",
        "- l'opacité des arrêtes entre les villes est déterminée par le nombre de lettres qui transitent entre deux villes.\n",
        "\n",
        "La cartographie est réalisée avec Folium, un port Python de la librairie Leaflet sous Javascript.\n",
        "\n",
        "\n",
        "#### 1. Extraction de données\n",
        "\n",
        "Le modèle de données pour nos nœuds est le suivant:\n",
        "```python\n",
        "{ \n",
        "    \"@xml:id 1\": <compteur d occurences> \n",
        "    , \"@xml:id 2\": <compteur d occurences>\n",
        "    , #...\n",
        "}\n",
        "```\n",
        "\n",
        "Le modèle pour le graphe est le suivant:\n",
        "```python\n",
        "[ \n",
        "    {\n",
        "        \"a\": \"ville1\", \n",
        "        \"b\": \"ville2\", \n",
        "        \"count\": <compteur d occurrences> \n",
        "    }\n",
        "    , {\n",
        "        \"a\": \"ville2\", \n",
        "        \"b\": \"ville3\", \n",
        "        \"count\": <compteur d occurrences> \n",
        "    }\n",
        "    , #...  \n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91d3b9c6",
      "metadata": {
        "scrolled": false,
        "id": "91d3b9c6"
      },
      "outputs": [],
      "source": [
        "# on parse tous les fichiers XML et on extrait tous les \n",
        "# @xml:id des lieux d'expédition/destination du `correspAction`\n",
        "# afin de construire nodes et edges. dans les deux cas, on ne \n",
        "# traite pas les index ayant pour valeur `na`, puisqu'ils ne sont\n",
        "# pas géoréférencés\n",
        "nodes = {}\n",
        "edges = []\n",
        "geojson_files = [ os.path.splitext(os.path.basename(fp))[0] \n",
        "                  for fp in os.listdir(os.path.join(WEB, \"json\")) ]  # liste de noms de geojson sans extension\n",
        "\n",
        "for fpath in CORPUS:\n",
        "    tree = etree.parse(fpath, parser=PARSER)\n",
        "    indexes = tree.xpath(\".//tei:correspAction/tei:placeName/@ref\", namespaces=NS_TEI)\n",
        "    indexes = [ idx.replace(\"#\", \"\") for idx in indexes ]  # on supprime le `#` au début pour retrouver l'@xml:id\n",
        "        \n",
        "    # d'abord, on remplit `nodes`: \n",
        "    for idx in indexes:\n",
        "        if idx not in nodes.keys() and idx != \"na\":\n",
        "            nodes[idx] = 1\n",
        "        elif idx != \"na\":\n",
        "            nodes[idx] += 1\n",
        "                \n",
        "    # ensuite, on remplit `edges`.\n",
        "    # on utilise `range` qui à chaque itération émet un index de `edges` =>\n",
        "    # permet d'itérer à travers tous les items de `edges`.\n",
        "    # on vérifie dans les 2 sens si `indexes` a déjà une entrée dans `edges`\n",
        "    # si oui, on incrémente le compteur. sinon, on ajoute une nouvelle entrée\n",
        "    # à `edges` pour représenter la nouvelle relation entre deux villes.\n",
        "    # on ne traite une correspondance que si la ville A et la ville B sont géoréférencées\n",
        "    if all(i in geojson_files for i in indexes):\n",
        "        sender, receiver = indexes\n",
        "            \n",
        "        # si la relation a<->b n'existe pas, on l'ajoute\n",
        "        if not any( [sender, receiver] == [edge[\"a\"], edge[\"b\"]] for edge in edges ):\n",
        "            edges.append({ 'a': sender, \"b\": receiver, \"count\": 1 })\n",
        "        # sinon, on incrémente le compteur\n",
        "        else:\n",
        "            for i in range(len(edges)):\n",
        "                if [sender, receiver] == [edges[i][\"a\"], edges[i][\"b\"]]:\n",
        "                    edges[i][\"count\"] += 1\n",
        "\n",
        "print(nodes)\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10983bca",
      "metadata": {
        "id": "10983bca"
      },
      "source": [
        "Petite complexité supplémentaire: `edges` est pour le moment une relation orientée: il peut y avoir une entrée de la liste où `['a': 'Paris', 'b': 'Kobe']` et une autre où `['a': 'Kobe', 'b': 'Paris']`, chacune avec son compteur. On croise donc ces deux entrées et additionne les compteurs pour que `edges` représente des relations non-dirigées.\n",
        "\n",
        "D'un point de vue technique, cela revient à intérer deux fois sur `edges` pour faire la comparaison entre les objets émis par les deux itérations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb3cd0a",
      "metadata": {
        "scrolled": true,
        "id": "7fb3cd0a"
      },
      "outputs": [],
      "source": [
        "edges_undirected = []  # variable pour stocker le graphe non dirigé\n",
        "\n",
        "for edge in edges:\n",
        "    for i in range(len(edges)):\n",
        "        if [ edge[\"a\"], edge[\"b\"] ] == [ edges[i][\"b\"], edges[i][\"a\"] ]:\n",
        "            edge[\"count\"] += edges[i][\"count\"]\n",
        "    edges_undirected.append(edge)\n",
        "edges = edges_undirected\n",
        "\n",
        "print(edges)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d4e827e",
      "metadata": {
        "id": "9d4e827e"
      },
      "source": [
        "#### 2. Construction de la carte\n",
        "\n",
        "On crée un objet `folium.Map` et on y ajoute d'abord nos nœuds, puis nos arrêtes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecbbb23b",
      "metadata": {
        "scrolled": true,
        "id": "ecbbb23b"
      },
      "outputs": [],
      "source": [
        "map = folium.Map(location=[48.8534951, 2.3483915], tiles=\"Stamen Toner\")\n",
        "markers = {}  # dictionnaire mappant un @xml:id à un objet `folium.CircleMarker`. sera utilisé pour construire les relations entre les villes\n",
        "node_titles = {}  # dictionnaire mappant l'@xml:id d'un lieu à son nom lisible\n",
        "    \n",
        "# on ajoute d'abord nos nœuds sur la carte\n",
        "for k, v in nodes.items():\n",
        "    # on ne traite que les clés qui ont un geojson, soit des infos géographiques\n",
        "    # attachées.\n",
        "    if k not in geojson_files:\n",
        "        print(f\"pas de geojson pour l'@xml:id: {k}. ce lieu n'est pas traité\")\n",
        "        continue\n",
        "\n",
        "    # on charge tous nos fichiers geojson. ils contiennent toutes\n",
        "    # les infos dont on a besoin (et bien plus)! on s'en sert pour extraire\n",
        "    # des géocoordonnées et le nom de l'endroit. on pourrait directement afficher\n",
        "    # un point sur la carte en ajoutant le geojson à notre carte leaflet, mais\n",
        "    # celui-ci aurait un diamètre fixe (alors qu'on veut un diamètre adapté\n",
        "    # au nombre de lettres associées au lieu) => on extrait des infos pour\n",
        "    # créer un `CircleMarker` folium\n",
        "    with open(os.path.join(WEB, \"json\", f\"{k}.geojson\"), mode=\"r\") as fh:\n",
        "        geojson = json.load(fh)                                   # on ouvre le fichier geojson \n",
        "    coordinates = geojson[\"features\"][0][\"geometry\"][\"coordinates\"]  # géocoordonnées\n",
        "    title = geojson[\"features\"][0][\"properties\"][\"display_name\"]     # nom complet\n",
        "        \n",
        "    node_titles[k] = title\n",
        "        \n",
        "    # pour garantir la lisibilité, on représente v (nombre de lettres liées à\n",
        "    # un endroit) sur une échelle logarithmique et on multiplie cette échelle\n",
        "    # logarithmique par 5. cela permet d'éviter que les gros marqueurs bloquent\n",
        "    # toute la carte et que les petits soient invisibles, tout en conservant\n",
        "    # un ordre de grandeur\n",
        "    if v > 1:\n",
        "        vlog = math.log(v) * 5\n",
        "    else:\n",
        "        vlog = v * 5\n",
        "        \n",
        "    markers[k] = folium.CircleMarker(\n",
        "        location=[ coordinates[1], coordinates[0] ]          # positionnement\n",
        "        , radius=vlog                                        # taille du marqueur\n",
        "        , color=COLORS[\"plum\"]                               # couleur de bordure\n",
        "        , fill_color=COLORS[\"gold\"]                          # couleur de remplissage\n",
        "        , fill_opacity=1                                     # opacité\n",
        "        , popup=f\"<b>{title}</b>: <br/><br/> <b>{v}</b> lettres reçues ou expédiées\"  # popup s'affichant quand on clicke sur le marker\n",
        "    )\n",
        "    markers[k].add_to(map)\n",
        "    \n",
        "# ensuite, on ajoute nos arrêtes: les relations entre 2 villes\n",
        "maxcount = max([ e[\"count\"] for e in edges ])  # +gd nombre de relations\n",
        "for edge in edges:\n",
        "    # on définit l'opacité en fonction du nombre de lettres envoyées: \n",
        "    # 0.3 + 0.7 x <proportion du nombre de lettres entre les 2 villes actuelles \n",
        "    #              par rapport au nombre maximal de lettres envoyées entre 2 villes>\n",
        "    opacity = (edge[\"count\"] / maxcount) * 0.7 + 0.3\n",
        "    folium.PolyLine(\n",
        "        locations=[ markers[edge[\"a\"]].location, markers[edge[\"b\"]].location ]  # positions des 2 villes\n",
        "        , color=COLORS[\"plum\"]\n",
        "        , stroke=5\n",
        "        , opacity=opacity\n",
        "        , fillColor=COLORS[\"plum\"]\n",
        "        , fillOpacity=opacity\n",
        "        , popup=f\"<b>{edge['count']}</b> lettres échangées entre <b>{node_titles[edge['a']]}</b> et <b>{node_titles[edge['b']]}</b>\"\n",
        "        , tooltip=f\"<b>{node_titles[edge['a']]}</b> <br/><br/> <b>{node_titles[edge['b']]}</b>\"\n",
        "    ).add_to(map)\n",
        "\n",
        "map  # tadaaaaaa\n",
        "# map.save(os.path.join(WEB, \"map.html\"))  # pour la voir en grand écran, décommenter puis aller ouvrir ce fichier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fef109a",
      "metadata": {
        "id": "7fef109a"
      },
      "source": [
        "## Pour conclure\n",
        "\n",
        "En partant d'un corpus en XML tout simple, on voit comment on peut arriver très vite à mettre au point une **pipeline d'édition scientifique**, avec des améliorations progressives et automatiques de l'encodage. Pour celles-ci, il n'est pas directement nécessaire de déployer l'artillerie lourde, faire un encodage plus qualitatif de nos métadonnées est possible de façon relativement simple. Reste ensuite à traiter le corps des lettres, que l'on a sagement laissé de côté.\n",
        "\n",
        "Quand on croise l'édition TEI avec des visualisations, on voit que des résultats assez arrivent assez facilement, et permettent de voir de façon synthétique l'évolution du corpus. Par contre, **la temporalité du corpus** n'a pas été prise en compte. Il serait intéressant de voir comment la spatialité et les interaction évoluent au fil du temps, mais mettre des filtres temporels aurait demandé de rajouter du Javascript etc, ce qui aurait été un peu compliqué.\n",
        "\n",
        "Si vous avez encore le courage: dans [`src/txt2xml.py`](https://github.com/paulhectork/cours_ens2023_xmltei/blob/main/src/txt2xml.py) est présentée une pipeline pour la production automatique du corpus en TEI à partir d'une version texte brut. C'est avec cette pipeline que le corpus sur lequel s'est basé cet atelier a été construite : )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "70e6d6b7",
        "6a5d3f60",
        "de6a055d",
        "7c91c3f9",
        "95076278",
        "2587b842",
        "ceaaeda1",
        "6960f043",
        "7fef109a"
      ]
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}